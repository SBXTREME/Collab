{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SBXTREME/Collab/blob/main/talk_to_your_pdf_without_KG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install dependencies\n",
        "!pip install langchain sentence-transformers PyPDF2 faiss-cpu"
      ],
      "metadata": {
        "collapsed": true,
        "id": "W7ehOjmn32BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import PyPDF2\n",
        "import faiss\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('secretkey')\n",
        "\n",
        "# 1. Load all PDFs from the folder\n",
        "def load_all_pdfs(folder_path):\n",
        "    all_text = \"\"\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            pdf_path = os.path.join(folder_path, filename)\n",
        "            with open(pdf_path, 'rb') as f:\n",
        "                reader = PyPDF2.PdfReader(f)\n",
        "                for page in reader.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        all_text += page_text + \"\\n\"\n",
        "    return all_text\n",
        "\n",
        "pdf_folder = '/content/sample_data/data'\n",
        "raw_text = load_all_pdfs(pdf_folder)\n",
        "\n",
        "# 2. Split text into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "docs = splitter.split_text(raw_text)\n",
        "\n",
        "# 3. Embed chunks\n",
        "embedder = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
        "doc_embeddings = embedder.encode(docs, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# 4. Build FAISS index\n",
        "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "# 5. Cosine similarity\n",
        "def cosine_similarity(a, b):\n",
        "    a = a / np.linalg.norm(a)\n",
        "    b = b / np.linalg.norm(b)\n",
        "    return np.dot(a, b)\n",
        "\n",
        "# 6. Retrieval with similarity threshold\n",
        "def retrieve_relevant_chunks(query, k=4, threshold=0.7):\n",
        "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(query_emb, k)\n",
        "    retrieved = []\n",
        "    for idx, dist in zip(I[0], D[0]):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        chunk_emb = doc_embeddings[idx]\n",
        "        sim = cosine_similarity(query_emb[0], chunk_emb)\n",
        "        if sim >= threshold:\n",
        "            retrieved.append((docs[idx], sim))\n",
        "    return [chunk for chunk, sim in retrieved]\n",
        "\n",
        "# 7. Strict system prompt\n",
        "STRICT_SYSTEM_PROMPT = (\n",
        "    \"You are a helpful and kind AI assistant. \"\n",
        "    \"You must only answer using the provided context from the PDFs. \"\n",
        "    \"If the answer is not in the context, say: \"\n",
        "    \"'Sorry, I couldn't find information about your question in the provided PDFs.' \"\n",
        "    \"Do not use any external knowledge. \"\n",
        "    \"If asked to summarize or provide a gist, only use the PDF content.\"\n",
        ")\n",
        "\n",
        "# 8. LLM API call\n",
        "def ask_llm(question, context):\n",
        "    url = \"https://api.generative.engine.capgemini.com/v2/llm/invoke\"\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"x-api-key\": API_KEY\n",
        "    }\n",
        "    context = context[:2000]\n",
        "    prompt = (\n",
        "        f\"Use ONLY the following context to answer the question. \"\n",
        "        f\"If the answer is not in the context, say you don't know.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
        "    )\n",
        "    payload = {\n",
        "        \"action\": \"run\",\n",
        "        \"modelInterface\": \"langchain\",\n",
        "        \"data\": {\n",
        "            \"mode\": \"chain\",\n",
        "            \"text\": prompt,\n",
        "            \"files\": [],\n",
        "            \"modelName\": \"openai.gpt-4o\",\n",
        "            \"provider\": \"azure\",\n",
        "            \"systemPrompt\": STRICT_SYSTEM_PROMPT,\n",
        "            \"sessionId\": \"123e4567-e89b-12d3-a456-426614174000\",\n",
        "            \"modelKwargs\": {\n",
        "                \"maxTokens\": 512,\n",
        "                \"temperature\": 0.0,\n",
        "                \"streaming\": False,\n",
        "                \"topP\": 0.9\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    try:\n",
        "        resp_json = response.json()\n",
        "        if 'content' in resp_json:\n",
        "            return resp_json['content']\n",
        "        elif 'data' in resp_json and 'output' in resp_json['data']:\n",
        "            return resp_json['data']['output']\n",
        "        elif 'output' in resp_json:\n",
        "            return resp_json['output']\n",
        "        elif 'message' in resp_json:\n",
        "            return f\"API Error: {resp_json['message']}\"\n",
        "        else:\n",
        "            return f\"Unexpected API response: {resp_json}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error parsing API response: {e}, raw: {response.text}\"\n",
        "\n",
        "# 9. Chat interface\n",
        "def chat():\n",
        "    print(\"Ask questions about the PDFs. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        q = input(\"\\nYour question: \")\n",
        "        if q.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Detect summary/gist requests\n",
        "        if any(word in q.lower() for word in [\"summarize\", \"summary\", \"gist\"]):\n",
        "            context = \"\\n\\n\".join(docs[:10])\n",
        "            print(\"\\nDEBUG: Using first 10 chunks for summary/gist.\\n\")\n",
        "            print(context)\n",
        "            answer = ask_llm(q, context)\n",
        "            print(\"\\nAnswer:\", answer)\n",
        "            continue\n",
        "\n",
        "        # Normal Q&A\n",
        "        chunks = retrieve_relevant_chunks(q, k=4, threshold=0.6)\n",
        "        if not chunks:\n",
        "            print(\"\\nAnswer: Sorry, I couldn't find information about your question in the provided PDFs.\")\n",
        "            continue\n",
        "        context = \"\\n\\n\".join(chunks)\n",
        "        print(\"\\nDEBUG: Retrieved context for your question:\\n\")\n",
        "        print(context)\n",
        "        answer = ask_llm(q, context)\n",
        "        print(\"=================================== Generated Response ===================================\")\n",
        "        print(\"\\nAnswer:\", answer)\n",
        "\n",
        "# 10. Run the chat interface\n",
        "chat()"
      ],
      "metadata": {
        "id": "xU_3NiqUdV91"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}